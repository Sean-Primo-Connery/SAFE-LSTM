{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-23T06:24:55.437312700Z",
     "start_time": "2024-07-23T06:24:49.652421400Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "class SAttn(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(SAttn, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.q = nn.Linear(self.input_dim, self.input_dim, bias=False)\n",
    "        self.k = nn.Linear(self.input_dim, self.input_dim, bias=False)\n",
    "        self.v = nn.Linear(self.input_dim, self.input_dim, bias=False)\n",
    "    def forward(self, input_tensor):\n",
    "        q = self.q(input_tensor)\n",
    "        k = self.k(input_tensor)\n",
    "        v = self.v(input_tensor)\n",
    "        attn = torch.matmul(q, k.permute(0, 2, 1))\n",
    "        attn = torch.softmax(attn, dim=2)\n",
    "        output = torch.matmul(attn, v)\n",
    "        return output"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-23T06:25:57.261025500Z",
     "start_time": "2024-07-23T06:25:57.229555400Z"
    }
   },
   "id": "d103360e38ad442b"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "class FFTLayer(nn.Module):\n",
    "    def __init__(self, seq_len, feat_size):\n",
    "        super(FFTLayer, self).__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.feat_size = feat_size\n",
    "        \n",
    "        self.complex_weight = nn.Parameter(torch.randn(1, self.seq_len//2+1, self.feat_size, 2, requires_grad=True))\n",
    "        \n",
    "    def forward(self, _x):\n",
    "        hidden_states=_x\n",
    "        x = torch.fft.rfft(hidden_states, n=self.seq_len, dim=1, norm='forward')\n",
    "        weight = torch.view_as_complex(self.complex_weight)\n",
    "        x = x*weight\n",
    "        suq_emb_fft = torch.fft.irfft(x, n=self.seq_len, dim=1, norm='forward')\n",
    "        \n",
    "        return suq_emb_fft"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-23T06:25:57.261025500Z",
     "start_time": "2024-07-23T06:25:57.245386500Z"
    }
   },
   "id": "be6ff6393decccc"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, seq_len=9, num_layers=2, dropout=0.):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.seq_len = seq_len\n",
    "        self.num_layers = num_layers\n",
    "        self.cat_dim = self.input_size+self.hidden_size\n",
    "        \n",
    "        # RNN\n",
    "        self.rnn = nn.LSTM(self.input_size, self.hidden_size, self.num_layers, batch_first=True)\n",
    "        \n",
    "        # FFT\n",
    "        self.fft = FFTLayer(seq_len, self.input_size)\n",
    "        self.weight_aggr = nn.Linear(self.seq_len, 1)\n",
    "        \n",
    "        # Att\n",
    "        self.EAtt = SAttn(self.cat_dim)\n",
    "        \n",
    "        # Prediction\n",
    "        self.fc1 = nn.Linear(self.cat_dim, self.input_size)\n",
    "        self.elu = nn.ELU()\n",
    "        self.fc2 = nn.Linear(self.input_size, self.output_size)\n",
    "    def forward(self, _x):\n",
    "        # RNN: Trend\n",
    "        f_trend, _ = self.rnn(_x)\n",
    "        \n",
    "        # FFT: Cycle\n",
    "        f_cycle = self.fft(_x)\n",
    "        \n",
    "        # Att\n",
    "        combine_feature = torch.cat([f_trend, f_cycle], dim=-1)\n",
    "        combine_feature = self.EAtt(combine_feature)\n",
    "        combine_feature = self.weight_aggr(combine_feature.permute(0,2,1)).flatten(1)\n",
    "        \n",
    "        # Prediction\n",
    "        hn = self.fc1(combine_feature)\n",
    "        hn = self.elu(hn)\n",
    "        hn = self.fc2(hn)\n",
    "        return hn"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-23T06:25:57.292284600Z",
     "start_time": "2024-07-23T06:25:57.261025500Z"
    }
   },
   "id": "10094a57adb907c0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
